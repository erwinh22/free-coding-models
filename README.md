<p align="center">
  <img src="https://img.shields.io/npm/v/free-coding-models?color=76b900&label=npm&logo=npm" alt="npm version">
  <img src="https://img.shields.io/node/v/free-coding-models?color=76b900&logo=node.js" alt="node version">
  <img src="https://img.shields.io/npm/l/free-coding-models?color=76b900" alt="license">
  <img src="https://img.shields.io/badge/models-150-76b900?logo=nvidia" alt="models count">
  <img src="https://img.shields.io/badge/providers-19-blue" alt="providers count">
</p>

<h1 align="center">free-coding-models</h1>

<p align="center">
  <strong>Contributors</strong><br>
  <a href="https://github.com/vava-nessa"><img src="https://avatars.githubusercontent.com/u/5466264?v=4&s=60" width="60" height="60" style="border-radius:50%" alt="vava-nessa"></a>
  <a href="https://github.com/erwinh22"><img src="https://avatars.githubusercontent.com/u/6641858?v=4&s=60" width="60" height="60" style="border-radius:50%" alt="erwinh22"></a>
  <a href="https://github.com/whit3rabbit"><img src="https://avatars.githubusercontent.com/u/12357518?v=4&s=60" width="60" height="60" style="border-radius:50%" alt="whit3rabbit"></a>
  <a href="https://github.com/skylaweber"><img src="https://avatars.githubusercontent.com/u/172871734?v=4&s=60" width="60" height="60" style="border-radius:50%" alt="skylaweber"></a>
  <br>
  <sub>
    <a href="https://github.com/vava-nessa">vava-nessa</a> &middot;
    <a href="https://github.com/erwinh22">erwinh22</a> &middot;
    <a href="https://github.com/whit3rabbit">whit3rabbit</a> &middot;
    <a href="https://github.com/skylaweber">skylaweber</a>
  </sub>
</p>

<p align="center">
  ğŸ’¬ <a href="https://discord.gg/5MbTnDC3Md">Let's talk about the project on Discord</a>
</p>

<p align="center">

```
1. Create a free API key (NVIDIA, OpenRouter, Hugging Face, etc.)
2. npm i -g free-coding-models
3. free-coding-models
```

</p>

<p align="center">
  <strong>Find the fastest coding LLM models in seconds</strong><br>
  <sub>Ping free coding models from 19 providers in real-time â€” pick the best one for OpenCode, OpenClaw, or any AI coding assistant</sub>
</p>

<p align="center">
  <img src="demo.gif" alt="free-coding-models demo" width="100%">
</p>

<p align="center">
  <a href="#-features">Features</a> â€¢
  <a href="#-requirements">Requirements</a> â€¢
  <a href="#-installation">Installation</a> â€¢
  <a href="#-usage">Usage</a> â€¢
  <a href="#-tui-columns">Columns</a> â€¢
  <a href="#-stability-score">Stability</a> â€¢
  <a href="#-coding-models">Models</a> â€¢
  <a href="#-opencode-integration">OpenCode</a> â€¢
  <a href="#-openclaw-integration">OpenClaw</a> â€¢
  <a href="#-how-it-works">How it works</a>
</p>

---

## âœ¨ Features

- **ğŸ¯ Coding-focused** â€” Only LLM models optimized for code generation, not chat or vision
- **ğŸŒ Multi-provider** â€” Models from NVIDIA NIM, Groq, Cerebras, SambaNova, OpenRouter, Hugging Face Inference, Replicate, DeepInfra, Fireworks AI, Codestral, Hyperbolic, Scaleway, Google AI, SiliconFlow, Together AI, Cloudflare Workers AI, Perplexity API, and ZAI
- **âš™ï¸ Settings screen** â€” Press `P` to manage provider API keys, enable/disable providers, test keys live, and manually check/install updates
- **ğŸš€ Parallel pings** â€” All models tested simultaneously via native `fetch`
- **ğŸ“Š Real-time animation** â€” Watch latency appear live in alternate screen buffer
- **ğŸ† Smart ranking** â€” Top 3 fastest models highlighted with medals ğŸ¥‡ğŸ¥ˆğŸ¥‰
- **â± Continuous monitoring** â€” Pings all models every 3 seconds forever, never stops
- **ğŸ“ˆ Rolling averages** â€” Avg calculated from ALL successful pings since start
- **ğŸ“Š Uptime tracking** â€” Percentage of successful pings shown in real-time
- **ğŸ“ Stability score** â€” Composite 0â€“100 score measuring consistency (p95, jitter, spikes, uptime) â€” a model with 400ms avg and stable responses beats a 250ms avg model that randomly spikes to 6s
- **ğŸ”„ Auto-retry** â€” Timeout models keep getting retried, nothing is ever "given up on"
- **ğŸ® Interactive selection** â€” Navigate with arrow keys directly in the table, press Enter to act
- **ğŸ”€ Startup mode menu** â€” Choose between OpenCode and OpenClaw before the TUI launches
- **ğŸ’» OpenCode integration** â€” Auto-detects NIM setup, sets model as default, launches OpenCode
- **ğŸ¦ OpenClaw integration** â€” Sets selected model as default provider in `~/.openclaw/openclaw.json`
- **ğŸ“ Feature Request (J key)** â€” Send anonymous feedback directly to the project team via a full-screen overlay with multi-line input (includes anonymous OS/terminal metadata in message footer only)
- **ğŸ¨ Clean output** â€” Zero scrollback pollution, interface stays open until Ctrl+C
- **ğŸ“¶ Status indicators** â€” UP âœ… Â· No Key ğŸ”‘ Â· Timeout â³ Â· Overloaded ğŸ”¥ Â· Not Found ğŸš«
- **ğŸ” Keyless latency** â€” Models are pinged even without an API key â€” a `ğŸ”‘ NO KEY` status confirms the server is reachable with real latency shown, so you can compare providers before committing to a key
- **ğŸ· Tier filtering** â€” Filter models by tier letter (S, A, B, C) with `--tier` flag or dynamically with `T` key
- **â­ Persistent favorites** â€” Press `F` on a selected row to pin/unpin it; favorites stay at top with a dark orange background and a star before the model name
- **ğŸ“Š Privacy-first analytics (optional)** â€” anonymous PostHog events with explicit consent + opt-out

---

## ğŸ“‹ Requirements

Before using `free-coding-models`, make sure you have:

1. **Node.js 18+** â€” Required for native `fetch` API
2. **At least one free API key** â€” pick any or all of:
   - **NVIDIA NIM** â€” [build.nvidia.com](https://build.nvidia.com) â†’ Profile â†’ API Keys â†’ Generate
   - **Groq** â€” [console.groq.com/keys](https://console.groq.com/keys) â†’ Create API Key
   - **Cerebras** â€” [cloud.cerebras.ai](https://cloud.cerebras.ai) â†’ API Keys â†’ Create
   - **SambaNova** â€” [sambanova.ai/developers](https://sambanova.ai/developers) â†’ Developers portal â†’ API key (dev tier generous)
   - **OpenRouter** â€” [openrouter.ai/keys](https://openrouter.ai/keys) â†’ Create key (50 req/day, 20/min on `:free`)
   - **Hugging Face Inference** â€” [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) â†’ Access Tokens (free monthly credits)
   - **Replicate** â€” [replicate.com/account/api-tokens](https://replicate.com/account/api-tokens) â†’ Create token (dev quota)
   - **DeepInfra** â€” [deepinfra.com/login](https://deepinfra.com/login) â†’ Login â†’ API key (free dev tier)
   - **Fireworks AI** â€” [fireworks.ai](https://fireworks.ai) â†’ Settings â†’ Access Tokens ($1 free credits)
   - **Mistral Codestral** â€” [codestral.mistral.ai](https://codestral.mistral.ai) â†’ API Keys (30 req/min, 2000/day â€” phone required)
   - **Hyperbolic** â€” [app.hyperbolic.ai/settings](https://app.hyperbolic.ai/settings) â†’ API Keys ($1 free trial)
   - **Scaleway** â€” [console.scaleway.com/iam/api-keys](https://console.scaleway.com/iam/api-keys) â†’ IAM â†’ API Keys (1M free tokens)
   - **Google AI Studio** â€” [aistudio.google.com/apikey](https://aistudio.google.com/apikey) â†’ Get API key (free Gemma models, 14.4K req/day)
   - **SiliconFlow** â€” [cloud.siliconflow.cn/account/ak](https://cloud.siliconflow.cn/account/ak) â†’ API Keys (free-model quotas vary by model)
   - **Together AI** â€” [api.together.ai/settings/api-keys](https://api.together.ai/settings/api-keys) â†’ API Keys (credits/promotions vary)
   - **Cloudflare Workers AI** â€” [dash.cloudflare.com](https://dash.cloudflare.com) â†’ Create API token + set `CLOUDFLARE_ACCOUNT_ID` (Free: 10k neurons/day)
   - **Perplexity API** â€” [perplexity.ai/settings/api](https://www.perplexity.ai/settings/api) â†’ API Key (tiered limits by spend)
   - **ZAI** â€” [z.ai](https://z.ai) â†’ Get API key (Coding Plan subscription)
3. **OpenCode** *(optional)* â€” [Install OpenCode](https://github.com/opencode-ai/opencode) to use the OpenCode integration
4. **OpenClaw** *(optional)* â€” [Install OpenClaw](https://openclaw.ai) to use the OpenClaw integration

 > ğŸ’¡ **Tip:** You don't need all nineteen providers. One key is enough to get started. Add more later via the Settings screen (`P` key). Models without a key still show real latency (`ğŸ”‘ NO KEY`) so you can evaluate providers before signing up.

---

## ğŸ“¦ Installation

```bash
# npm (global install â€” recommended)
npm install -g free-coding-models

# pnpm
pnpm add -g free-coding-models

# bun
bun add -g free-coding-models

# Or use directly with npx/pnpx/bunx
npx free-coding-models YOUR_API_KEY
pnpx free-coding-models YOUR_API_KEY
bunx free-coding-models YOUR_API_KEY
```

---

## ğŸš€ Usage

```bash
# Just run it â€” shows a startup menu to pick OpenCode or OpenClaw, prompts for API key if not set
free-coding-models

# Explicitly target OpenCode CLI (TUI + Enter launches OpenCode CLI)
free-coding-models --opencode

# Explicitly target OpenCode Desktop (TUI + Enter sets model & opens Desktop app)
free-coding-models --opencode-desktop

# Explicitly target OpenClaw (TUI + Enter sets model as default in OpenClaw)
free-coding-models --openclaw

# Show only top-tier models (A+, S, S+)
free-coding-models --best

# Analyze for 10 seconds and output the most reliable model
free-coding-models --fiable

# Disable anonymous analytics for this run
free-coding-models --no-telemetry

# Filter models by tier letter
free-coding-models --tier S          # S+ and S only
free-coding-models --tier A          # A+, A, A- only
free-coding-models --tier B          # B+, B only
free-coding-models --tier C          # C only

# Combine flags freely
free-coding-models --openclaw --tier S
free-coding-models --opencode --best
```

### Startup mode menu

When you run `free-coding-models` without `--opencode` or `--openclaw`, you get an interactive startup menu:

```
  âš¡ Free Coding Models â€” Choose your tool

  â¯ ğŸ’» OpenCode CLI
       Press Enter on a model â†’ launch OpenCode CLI with it as default

    ğŸ–¥ OpenCode Desktop
       Press Enter on a model â†’ set model & open OpenCode Desktop app

    ğŸ¦ OpenClaw
       Press Enter on a model â†’ set it as default in OpenClaw config

  â†‘â†“ Navigate  â€¢  Enter Select  â€¢  Ctrl+C Exit
```

Use `â†‘â†“` arrows to select, `Enter` to confirm. Then the TUI launches with your chosen mode shown in the header badge.

**How it works:**
 1. **Ping phase** â€” All enabled models are pinged in parallel (up to 150 across 19 providers)
 2. **Continuous monitoring** â€” Models are re-pinged every 3 seconds forever
3. **Real-time updates** â€” Watch "Latest", "Avg", and "Up%" columns update live
4. **Select anytime** â€” Use â†‘â†“ arrows to navigate, press Enter on a model to act
5. **Smart detection** â€” Automatically detects if NVIDIA NIM is configured in OpenCode or OpenClaw

 Setup wizard (first run â€” walks through all 19 providers):

```
  ğŸ”‘ First-time setup â€” API keys
  Enter keys for any provider you want to use. Press Enter to skip one.

  â— NVIDIA NIM
    Free key at: https://build.nvidia.com
    Profile â†’ API Keys â†’ Generate
  Enter key (or Enter to skip): nvapi-xxxx

  â— Groq
    Free key at: https://console.groq.com/keys
    API Keys â†’ Create API Key
  Enter key (or Enter to skip): gsk_xxxx

  â— Cerebras
    Free key at: https://cloud.cerebras.ai
    API Keys â†’ Create
  Enter key (or Enter to skip):

  â— SambaNova
    Free key at: https://cloud.sambanova.ai/apis
    API Keys â†’ Create ($5 free trial, 3 months)
  Enter key (or Enter to skip):

  âœ… 2 key(s) saved to ~/.free-coding-models.json
  You can add or change keys anytime with the P key in the TUI.
```

You don't need all seventeen â€” skip any provider by pressing Enter. At least one key is required.

### Adding or changing keys later

Press **`P`** to open the Settings screen at any time:

```
  âš™  Settings

  Providers

  â¯ [ âœ… ] NVIDIA NIM              nvapi-â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢3f9a  [Test âœ…]  Free tier (provider quota by model)
    [ âœ… ] OpenRouter              (no key set)            [Test â€”]   50 req/day, 20/min (:free shared quota)
    [ âœ… ] Hugging Face Inference  (no key set)            [Test â€”]   Free monthly credits (~$0.10)

  Setup Instructions â€” NVIDIA NIM
  1) Create a NVIDIA NIM account: https://build.nvidia.com
  2) Profile â†’ API Keys â†’ Generate
  3) Press T to test your key

  â†‘â†“ Navigate  â€¢  Enter Edit key / Check-or-Install update  â€¢  Space Toggle enabled  â€¢  T Test key  â€¢  U Check updates  â€¢  Esc Close
```

- **â†‘â†“** â€” navigate providers
- **Enter** â€” enter inline key edit mode (type your key, Enter to save, Esc to cancel)
- **Space** â€” toggle provider enabled/disabled
- **T** â€” fire a real test ping to verify the key works (shows âœ…/âŒ)
- **U** â€” manually check npm for a newer version
- **Esc** â€” close settings and reload models list

Keys are saved to `~/.free-coding-models.json` (permissions `0600`).

Analytics toggle is in the same Settings screen (`P`) as a dedicated row (toggle with Enter or Space).
Manual update is in the same Settings screen (`P`) under **Maintenance** (Enter to check, Enter again to install when an update is available).
Favorites are also persisted in the same config file and survive restarts.

### Environment variable overrides

Env vars always take priority over the config file:

```bash
NVIDIA_API_KEY=nvapi-xxx free-coding-models
GROQ_API_KEY=gsk_xxx free-coding-models
CEREBRAS_API_KEY=csk_xxx free-coding-models
OPENROUTER_API_KEY=sk-or-xxx free-coding-models
HUGGINGFACE_API_KEY=hf_xxx free-coding-models
REPLICATE_API_TOKEN=r8_xxx free-coding-models
DEEPINFRA_API_KEY=di_xxx free-coding-models
FIREWORKS_API_KEY=fw_xxx free-coding-models
SILICONFLOW_API_KEY=sk_xxx free-coding-models
TOGETHER_API_KEY=together_xxx free-coding-models
CLOUDFLARE_API_TOKEN=cf_xxx CLOUDFLARE_ACCOUNT_ID=your_account_id free-coding-models
PERPLEXITY_API_KEY=pplx_xxx free-coding-models
ZAI_API_KEY=zai-xxx free-coding-models
FREE_CODING_MODELS_TELEMETRY=0 free-coding-models
```

Telemetry env vars:

- `FREE_CODING_MODELS_TELEMETRY=0|1` â€” force disable/enable analytics
- `FREE_CODING_MODELS_POSTHOG_KEY` â€” PostHog project API key (required to send events)
- `FREE_CODING_MODELS_POSTHOG_HOST` â€” optional ingest host (`https://eu.i.posthog.com` default)
- `FREE_CODING_MODELS_TELEMETRY_DEBUG=1` â€” optional stderr debug logs for telemetry troubleshooting

On first run (or when consent policy changes), the CLI asks users to accept or decline anonymous analytics.
When enabled, telemetry events include: event name, app version, selected mode, system (`macOS`/`Windows`/`Linux`), and terminal family (`Terminal.app`, `iTerm2`, `kitty`, `Warp`, `WezTerm`, etc., with generic fallback from `TERM_PROGRAM`/`TERM`).

### Get your free API keys

**NVIDIA NIM** (44 models, S+ â†’ C tier):
1. Sign up at [build.nvidia.com](https://build.nvidia.com)
2. Go to Profile â†’ API Keys â†’ Generate API Key
3. Name it (e.g. "free-coding-models"), set expiry to "Never"
4. Copy â€” shown only once!

**Groq** (6 models, fast inference):
1. Sign up at [console.groq.com](https://console.groq.com)
2. Go to API Keys â†’ Create API Key

**Cerebras** (3 models, ultra-fast silicon):
1. Sign up at [cloud.cerebras.ai](https://cloud.cerebras.ai)
2. Go to API Keys â†’ Create

**OpenRouter** (`:free` models):
1. Sign up at [openrouter.ai/keys](https://openrouter.ai/keys)
2. Create API key (`sk-or-...`)

**Hugging Face Inference**:
1. Sign up at [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
2. Create Access Token (`hf_...`)

**Replicate**:
1. Sign up at [replicate.com/account/api-tokens](https://replicate.com/account/api-tokens)
2. Create API token (`r8_...`)

**DeepInfra**:
1. Sign up at [deepinfra.com/login](https://deepinfra.com/login)
2. Create API key from your account dashboard

**Fireworks AI**:
1. Sign up at [fireworks.ai](https://fireworks.ai)
2. Open Settings â†’ Access Tokens and create a token

**Mistral Codestral**:
1. Sign up at [codestral.mistral.ai](https://codestral.mistral.ai)
2. Go to API Keys â†’ Create

**Hyperbolic**:
1. Sign up at [app.hyperbolic.ai/settings](https://app.hyperbolic.ai/settings)
2. Create an API key in Settings

**Scaleway**:
1. Sign up at [console.scaleway.com/iam/api-keys](https://console.scaleway.com/iam/api-keys)
2. Go to IAM â†’ API Keys

**Google AI Studio**:
1. Sign up at [aistudio.google.com/apikey](https://aistudio.google.com/apikey)
2. Create an API key for Gemini/Gemma endpoints

**SiliconFlow**:
1. Sign up at [cloud.siliconflow.cn/account/ak](https://cloud.siliconflow.cn/account/ak)
2. Create API key in Account â†’ API Keys

**Together AI**:
1. Sign up at [api.together.ai/settings/api-keys](https://api.together.ai/settings/api-keys)
2. Create an API key in Settings

**Cloudflare Workers AI**:
1. Sign up at [dash.cloudflare.com](https://dash.cloudflare.com)
2. Create an API token with Workers AI permissions
3. Export both `CLOUDFLARE_API_TOKEN` and `CLOUDFLARE_ACCOUNT_ID`

**Perplexity API**:
1. Sign up at [perplexity.ai/settings/api](https://www.perplexity.ai/settings/api)
2. Create API key (`PERPLEXITY_API_KEY`)

**ZAI** (5 models, GLM family):
1. Sign up at [z.ai](https://z.ai)
2. Subscribe to Coding Plan
3. Get API key from dashboard

> ğŸ’¡ **Free tiers** â€” each provider exposes a dev/free tier with its own quotas. ZAI requires a Coding Plan subscription.

---

## ğŸ¤– Coding Models

**150 coding models** across 19 providers and 8 tiers, ranked by [SWE-bench Verified](https://www.swebench.com) â€” the industry-standard benchmark measuring real GitHub issue resolution. Scores are self-reported by providers unless noted.

### ZAI Coding Plan (5 models)

| Tier | SWE-bench | Model |
|------|-----------|-------|
| **S+** â‰¥70% | GLM-5 (77.8%), GLM-4.5 (75.0%), GLM-4.7 (73.8%), GLM-4.5-Air (72.0%), GLM-4.6 (70.0%) |

### NVIDIA NIM (44 models)

| Tier | SWE-bench | Models |
|------|-----------|--------|
| **S+** â‰¥70% | GLM 5 (77.8%), Kimi K2.5 (76.8%), Step 3.5 Flash (74.4%), MiniMax M2.1 (74.0%), GLM 4.7 (73.8%), DeepSeek V3.2 (73.1%), Devstral 2 (72.2%), Kimi K2 Thinking (71.3%), Qwen3 Coder 480B (70.6%), Qwen3 235B (70.0%) |
| **S** 60â€“70% | MiniMax M2 (69.4%), DeepSeek V3.1 Terminus (68.4%), Qwen3 80B Thinking (68.0%), Qwen3.5 400B (68.0%), Kimi K2 Instruct (65.8%), Qwen3 80B Instruct (65.0%), DeepSeek V3.1 (62.0%), Llama 4 Maverick (62.0%), GPT OSS 120B (60.0%) |
| **A+** 50â€“60% | Mistral Large 675B (58.0%), Nemotron Ultra 253B (56.0%), Colosseum 355B (52.0%), QwQ 32B (50.0%) |
| **A** 40â€“50% | Nemotron Super 49B (49.0%), Mistral Medium 3 (48.0%), Qwen2.5 Coder 32B (46.0%), Magistral Small (45.0%), Llama 4 Scout (44.0%), Llama 3.1 405B (44.0%), Nemotron Nano 30B (43.0%), R1 Distill 32B (43.9%), GPT OSS 20B (42.0%) |
| **A-** 35â€“40% | Llama 3.3 70B (39.5%), Seed OSS 36B (38.0%), R1 Distill 14B (37.7%), Stockmark 100B (36.0%) |
| **B+** 30â€“35% | Ministral 14B (34.0%), Mixtral 8x22B (32.0%), Granite 34B Code (30.0%) |
| **B** 20â€“30% | R1 Distill 8B (28.2%), R1 Distill 7B (22.6%) |
| **C** <20% | Gemma 2 9B (18.0%), Phi 4 Mini (14.0%), Phi 3.5 Mini (12.0%) |

### Groq (10 models)

| Tier | SWE-bench | Model |
|------|-----------|-------|
| **S** 60â€“70% | Kimi K2 Instruct (65.8%), Llama 4 Maverick (62.0%) |
| **A+** 50â€“60% | QwQ 32B (50.0%) |
| **A** 40â€“50% | Llama 4 Scout (44.0%), R1 Distill 70B (43.9%) |
| **A-** 35â€“40% | Llama 3.3 70B (39.5%) |

### Cerebras (7 models)

| Tier | SWE-bench | Model |
|------|-----------|-------|
| **A+** 50â€“60% | Qwen3 32B (50.0%) |
| **A** 40â€“50% | Llama 4 Scout (44.0%) |
| **A-** 35â€“40% | Llama 3.3 70B (39.5%) |

### Tier scale

- **S+/S** â€” Elite frontier coders (â‰¥60% SWE-bench), best for complex real-world tasks and refactors
- **A+/A** â€” Great alternatives, strong at most coding tasks
- **A-/B+** â€” Solid performers, good for targeted programming tasks
- **B/C** â€” Lightweight or older models, good for code completion on constrained infra

### Filtering by tier

Use `--tier` to focus on a specific capability band:

```bash
free-coding-models --tier S     # Only S+ and S (frontier models)
free-coding-models --tier A     # Only A+, A, A- (solid performers)
free-coding-models --tier B     # Only B+, B (lightweight options)
free-coding-models --tier C     # Only C (edge/minimal models)
```

#### Dynamic tier filtering with E/D keys

During runtime, use **E** and **D** keys to dynamically adjust the tier filter:

- **E** (Elevate) â€” Show fewer, higher-tier models (cycle: All â†’ S â†’ A â†’ B â†’ C â†’ All)
- **D** (Descend) â€” Show more, lower-tier models (cycle: All â†’ C â†’ B â†’ A â†’ S â†’ All)

Current tier filter is shown in the header badge (e.g., `[Tier S]`)

---

## ğŸ“Š TUI Columns

The main table displays one row per model with the following columns:

| Column | Sort key | Description |
|--------|----------|-------------|
| **Rank** | `R` | Position based on current sort order (medals for top 3: ğŸ¥‡ğŸ¥ˆğŸ¥‰) |
| **Tier** | `Y` | SWE-bench tier (S+, S, A+, A, A-, B+, B, C) |
| **SWE%** | `S` | SWE-bench Verified score â€” the industry-standard benchmark for real GitHub issue resolution |
| **CTX** | `C` | Context window size in thousands of tokens (e.g. `128k`) |
| **Model** | `M` | Model display name (favorites show â­ prefix) |
| **Origin** | `N` | Provider name (NIM, Groq, Cerebras, etc.) â€” press `N` to cycle origin filter |
| **Latest Ping** | `L` | Most recent round-trip latency in milliseconds |
| **Avg Ping** | `A` | Rolling average of ALL successful pings since launch |
| **Health** | `H` | Current status: UP âœ…, NO KEY ğŸ”‘, Timeout â³, Overloaded ğŸ”¥, Not Found ğŸš« |
| **Verdict** | `V` | Health verdict based on avg latency + stability analysis (see below) |
| **Stability** | `B` | Composite 0â€“100 consistency score (see [Stability Score](#-stability-score)) |
| **Up%** | `U` | Uptime â€” percentage of successful pings out of total attempts |

### Verdict values

The Verdict column combines average latency with stability analysis:

| Verdict | Meaning |
|---------|---------|
| **Perfect** | Avg < 400ms with stable p95/jitter |
| **Normal** | Avg < 1000ms, consistent responses |
| **Slow** | Avg 1000â€“2000ms |
| **Spiky** | Good avg but erratic tail latency (p95 >> avg) |
| **Very Slow** | Avg 2000â€“5000ms |
| **Overloaded** | Server returned 429/503 (rate limited or capacity hit) |
| **Unstable** | Was previously up but now timing out, or avg > 5000ms |
| **Not Active** | No successful pings yet |
| **Pending** | First ping still in flight |

---

## ğŸ“ Stability Score

The **Stability** column (sort with `B` key) shows a composite 0â€“100 score that answers: *"How consistent and predictable is this model?"*

Average latency alone is misleading â€” a model averaging 250ms that randomly spikes to 6 seconds *feels* slower in practice than a steady 400ms model. The stability score captures this.

### Formula

Four signals are normalized to 0â€“100 each, then combined with weights:

```
Stability = 0.30 Ã— p95_score
          + 0.30 Ã— jitter_score
          + 0.20 Ã— spike_score
          + 0.20 Ã— reliability_score
```

| Component | Weight | What it measures | How it's normalized |
|-----------|--------|-----------------|---------------------|
| **p95 latency** | 30% | Tail-latency spikes â€” the worst 5% of response times | `100 Ã— (1 - p95 / 5000)`, clamped to 0â€“100 |
| **Jitter (Ïƒ)** | 30% | Erratic response times â€” standard deviation of ping times | `100 Ã— (1 - jitter / 2000)`, clamped to 0â€“100 |
| **Spike rate** | 20% | Fraction of pings above 3000ms | `100 Ã— (1 - spikes / total_pings)` |
| **Reliability** | 20% | Uptime â€” fraction of successful HTTP 200 pings | Direct uptime percentage (0â€“100) |

### Color coding

| Score | Color | Interpretation |
|-------|-------|----------------|
| **80â€“100** | Green | Rock-solid â€” very consistent, safe to rely on |
| **60â€“79** | Cyan | Good â€” occasional variance but generally stable |
| **40â€“59** | Yellow | Shaky â€” noticeable inconsistency |
| **< 40** | Red | Unreliable â€” frequent spikes or failures |
| **â€”** | Dim | No data yet (no successful pings) |

### Example

Two models with similar average latency, very different real-world experience:

```
Model A:  avg 250ms,  p95 6000ms,  jitter 1800ms  â†’  Stability ~30  (red)
Model B:  avg 400ms,  p95  650ms,  jitter  120ms  â†’  Stability ~85  (green)
```

Model B is the better choice despite its higher average â€” it won't randomly stall your coding workflow.

> ğŸ’¡ **Tip:** Sort by Stability (`B` key) after a few minutes of monitoring to find the models that deliver the most predictable performance.

---

## ğŸ”Œ OpenCode Integration

**The easiest way** â€” let `free-coding-models` do everything:

1. **Run**: `free-coding-models --opencode` (or choose OpenCode from the startup menu)
2. **Wait** for models to be pinged (green âœ… status)
3. **Navigate** with â†‘â†“ arrows to your preferred model
4. **Press Enter** â€” tool automatically:
   - Detects if NVIDIA NIM is configured in OpenCode
   - Sets your selected model as default in `~/.config/opencode/opencode.json`
   - Launches OpenCode with the model ready to use

### tmux sub-agent panes

When launched from an existing `tmux` session, `free-coding-models` now auto-adds an OpenCode `--port` argument so OpenCode/oh-my-opencode can spawn sub-agents in panes.

- Priority 1: reuse `OPENCODE_PORT` if it is valid and free
- Priority 2: auto-pick the first free port in `4096-5095`

You can force a specific port:

```bash
OPENCODE_PORT=4098 free-coding-models --opencode
```

### ZAI provider proxy

OpenCode doesn't natively support ZAI's API path format (`/api/coding/paas/v4/*`). When you select a ZAI model, `free-coding-models` automatically starts a local reverse proxy that translates OpenCode's standard `/v1/*` requests to ZAI's API. This is fully transparent -- just select a ZAI model and press Enter.

**How it works:**
1. A localhost HTTP proxy starts on a random available port
2. OpenCode is configured with a `zai` provider pointing at `http://localhost:<port>/v1`
3. The proxy rewrites `/v1/models` to `/api/coding/paas/v4/models` and `/v1/chat/completions` to `/api/coding/paas/v4/chat/completions`
4. When OpenCode exits, the proxy shuts down automatically

No manual configuration needed -- the proxy lifecycle is managed entirely by `free-coding-models`.

### Manual OpenCode Setup (Optional)

Create or edit `~/.config/opencode/opencode.json`:

```json
{
  "provider": {
    "nvidia": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "NVIDIA NIM",
      "options": {
        "baseURL": "https://integrate.api.nvidia.com/v1",
        "apiKey": "{env:NVIDIA_API_KEY}"
      }
    }
  },
  "model": "nvidia/deepseek-ai/deepseek-v3.2"
}
```

Then set the environment variable:

```bash
export NVIDIA_API_KEY=nvapi-xxxx-your-key-here
# Add to ~/.bashrc or ~/.zshrc for persistence
```

Run `/models` in OpenCode and select **NVIDIA NIM** provider and your chosen model.

> âš ï¸ **Note:** Free models have usage limits based on NVIDIA's tier â€” check [build.nvidia.com](https://build.nvidia.com) for quotas.

### Automatic Installation Fallback

If NVIDIA NIM is not yet configured in OpenCode, the tool:
- Shows installation instructions in your terminal
- Creates a `prompt` file in `$HOME/prompt` with the exact configuration
- Launches OpenCode, which will detect and display the prompt automatically

---

## ğŸ¦ OpenClaw Integration

OpenClaw is an autonomous AI agent daemon. `free-coding-models` can configure it to use NVIDIA NIM models as its default provider â€” no download or local setup needed, everything runs via the NIM remote API.

### Quick Start

```bash
free-coding-models --openclaw
```

Or run without flags and choose **OpenClaw** from the startup menu.

1. **Wait** for models to be pinged
2. **Navigate** with â†‘â†“ arrows to your preferred model
3. **Press Enter** â€” tool automatically:
   - Reads `~/.openclaw/openclaw.json`
   - Adds the `nvidia` provider block (NIM base URL + your API key) if missing
   - Sets `agents.defaults.model.primary` to `nvidia/<model-id>`
   - Saves config and prints next steps

### What gets written to OpenClaw config

```json
{
  "models": {
    "providers": {
      "nvidia": {
        "baseUrl": "https://integrate.api.nvidia.com/v1",
        "api": "openai-completions"
      }
    }
  },
  "env": {
    "NVIDIA_API_KEY": "nvapi-xxxx-your-key"
  },
  "agents": {
    "defaults": {
      "model": {
        "primary": "nvidia/deepseek-ai/deepseek-v3.2"
      },
      "models": {
        "nvidia/deepseek-ai/deepseek-v3.2": {}
      }
    }
  }
}
```

> âš ï¸ **Note:** `providers` must be nested under `models.providers` â€” not at the config root. A root-level `providers` key is ignored by OpenClaw.

> âš ï¸ **Note:** The model must also be listed in `agents.defaults.models` (the allowlist). Without this entry, OpenClaw rejects the model with *"not allowed"* even if it is set as primary.

### After updating OpenClaw config

OpenClaw's gateway **auto-reloads** config file changes (depending on `gateway.reload.mode`). To apply manually:

```bash
# Apply via CLI
openclaw models set nvidia/deepseek-ai/deepseek-v3.2

# Or re-run the interactive setup wizard
openclaw configure
```

> âš ï¸ **Note:** `openclaw restart` does **not** exist as a CLI command. Kill and relaunch the process manually if you need a full restart.

> ğŸ’¡ **Why use remote NIM models with OpenClaw?** NVIDIA NIM serves models via a fast API â€” no local GPU required, no VRAM limits, free credits for developers. You get frontier-class coding models (DeepSeek V3, Kimi K2, Qwen3 Coder) without downloading anything.

### Patching OpenClaw for full NVIDIA model support

**Problem:** By default, OpenClaw only allows a few specific NVIDIA models in its allowlist. If you try to use a model that's not in the list, you'll get this error:

```
Model "nvidia/mistralai/devstral-2-123b-instruct-2512" is not allowed. Use /models to list providers, or /models <provider> to list models.
```

**Solution:** Patch OpenClaw's configuration to add ALL 47 NVIDIA models from `free-coding-models` to the allowlist:

```bash
# From the free-coding-models package directory
node patch-openclaw.js
```

This script:
- Backs up `~/.openclaw/agents/main/agent/models.json` and `~/.openclaw/openclaw.json`
- Adds all 47 NVIDIA models with proper context window and token limits
- Preserves existing models and configuration
- Prints a summary of what was added

**After patching:**

1. Restart OpenClaw gateway:
   ```bash
   systemctl --user restart openclaw-gateway
   ```

2. Verify models are available:
   ```bash
   free-coding-models --openclaw
   ```

3. Select any model â€” no more "not allowed" errors!

**Why this is needed:** OpenClaw uses a strict allowlist system to prevent typos and invalid models. The `patch-openclaw.js` script populates the allowlist with all known working NVIDIA models, so you can freely switch between them without manually editing config files.

---

## âš™ï¸ How it works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Enter alternate screen buffer (like vim/htop/less)           â”‚
â”‚  2. Ping ALL models in parallel                                  â”‚
â”‚  3. Display real-time table with Latest/Avg/Stability/Up%        â”‚
â”‚  4. Re-ping ALL models every 3 seconds (forever)               â”‚
â”‚  5. Update rolling averages + stability scores per model        â”‚
â”‚  6. User can navigate with â†‘â†“ and select with Enter            â”‚
â”‚  7. On Enter (OpenCode): set model, launch OpenCode             â”‚
â”‚  8. On Enter (OpenClaw): update ~/.openclaw/openclaw.json       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Result:** Continuous monitoring interface that stays open until you select a model or press Ctrl+C. Rolling averages give you accurate long-term latency data, the stability score reveals which models are truly consistent vs. deceptively spikey, and you can configure your tool of choice with one keystroke.

---

## ğŸ“‹ API Reference

**Environment variables (override config file):**

| Variable | Description |
|----------|-------------|
| `NVIDIA_API_KEY` | NVIDIA NIM key |
| `GROQ_API_KEY` | Groq key |
| `CEREBRAS_API_KEY` | Cerebras key |
| `SAMBANOVA_API_KEY` | SambaNova key |
| `OPENROUTER_API_KEY` | OpenRouter key |
| `HUGGINGFACE_API_KEY` / `HF_TOKEN` | Hugging Face token |
| `REPLICATE_API_TOKEN` | Replicate token |
| `DEEPINFRA_API_KEY` / `DEEPINFRA_TOKEN` | DeepInfra key |
| `CODESTRAL_API_KEY` | Mistral Codestral key |
| `HYPERBOLIC_API_KEY` | Hyperbolic key |
| `SCALEWAY_API_KEY` | Scaleway key |
| `GOOGLE_API_KEY` | Google AI Studio key |
| `SILICONFLOW_API_KEY` | SiliconFlow key |
| `TOGETHER_API_KEY` | Together AI key |
| `CLOUDFLARE_API_TOKEN` / `CLOUDFLARE_API_KEY` | Cloudflare Workers AI token/key |
| `CLOUDFLARE_ACCOUNT_ID` | Cloudflare account ID (required for Workers AI endpoint URL) |
| `PERPLEXITY_API_KEY` / `PPLX_API_KEY` | Perplexity API key |
| `ZAI_API_KEY` | ZAI key |
| `FREE_CODING_MODELS_TELEMETRY` | `0` disables analytics, `1` enables analytics |
| `FREE_CODING_MODELS_POSTHOG_KEY` | PostHog project API key used for anonymous event capture |
| `FREE_CODING_MODELS_POSTHOG_HOST` | Optional PostHog ingest host (`https://eu.i.posthog.com` default) |

**Config file:** `~/.free-coding-models.json` (created automatically, permissions `0600`)

```json
{
  "apiKeys": {
    "nvidia":   "nvapi-xxx",
    "groq":     "gsk_xxx",
    "cerebras": "csk_xxx",
    "openrouter": "sk-or-xxx",
    "huggingface": "hf_xxx",
    "replicate": "r8_xxx",
    "deepinfra": "di_xxx",
    "siliconflow": "sk_xxx",
    "together": "together_xxx",
    "cloudflare": "cf_xxx",
    "perplexity": "pplx_xxx",
    "zai":      "zai-xxx"
  },
  "providers": {
    "nvidia":   { "enabled": true },
    "groq":     { "enabled": true },
    "cerebras": { "enabled": true },
    "openrouter": { "enabled": true },
    "huggingface": { "enabled": true },
    "replicate": { "enabled": true },
    "deepinfra": { "enabled": true },
    "siliconflow": { "enabled": true },
    "together": { "enabled": true },
    "cloudflare": { "enabled": true },
    "perplexity": { "enabled": true },
    "zai":      { "enabled": true }
  },
  "favorites": [
    "nvidia/deepseek-ai/deepseek-v3.2"
  ],
  "telemetry": {
    "enabled": true,
    "consentVersion": 1,
    "anonymousId": "anon_550e8400-e29b-41d4-a716-446655440000"
  }
}
```

**Configuration:**
- **Ping timeout**: 15 seconds per attempt (slow models get more time)
- **Ping interval**: 3 seconds between complete re-pings of all models (adjustable with W/X keys)
- **Monitor mode**: Interface stays open forever, press Ctrl+C to exit

**Flags:**

| Flag | Description |
|------|-------------|
| *(none)* | Show startup menu to choose OpenCode or OpenClaw |
| `--opencode` | OpenCode CLI mode â€” Enter launches OpenCode CLI with selected model |
| `--opencode-desktop` | OpenCode Desktop mode â€” Enter sets model & opens OpenCode Desktop app |
| `--openclaw` | OpenClaw mode â€” Enter sets selected model as default in OpenClaw |
| `--best` | Show only top-tier models (A+, S, S+) |
| `--fiable` | Analyze 10 seconds, output the most reliable model as `provider/model_id` |
| `--no-telemetry` | Disable anonymous analytics for this run |
| `--tier S` | Show only S+ and S tier models |
| `--tier A` | Show only A+, A, A- tier models |
| `--tier B` | Show only B+, B tier models |
| `--tier C` | Show only C tier models |
| `--profile <name>` | Load a saved config profile on startup |
| `--recommend` | Auto-open Smart Recommend overlay on start |

**Keyboard shortcuts (main TUI):**
- **â†‘â†“** â€” Navigate models
- **Enter** â€” Select model (launches OpenCode or sets OpenClaw default, depending on mode)
- **R/Y/O/M/L/A/S/N/H/V/B/U** â€” Sort by Rank/Tier/Origin/Model/LatestPing/Avg/SWE/Ctx/Health/Verdict/Stability/Uptime
- **F** â€” Toggle favorite on selected model (â­ in Model column, pinned at top)
- **T** â€” Cycle tier filter (All â†’ S+ â†’ S â†’ A+ â†’ A â†’ A- â†’ B+ â†’ B â†’ C â†’ All)
- **Z** â€” Cycle mode (OpenCode CLI â†’ OpenCode Desktop â†’ OpenClaw)
- **P** â€” Open Settings (manage API keys, provider toggles, analytics toggle, manual update, profiles)
- **Shift+P** â€” Cycle through saved profiles (switches live TUI settings)
- **Shift+S** â€” Save current TUI settings as a named profile (inline prompt)
- **Q** â€” Open Smart Recommend overlay (find the best model for your task)
- **E** â€” Elevate tier filter (show higher tiers)
- **D** â€” Descend tier filter (show lower tiers)
- **W** â€” Decrease ping interval (faster pings)
- **X** â€” Increase ping interval (slower pings)
- **K** / **Esc** â€” Show/hide help overlay
- **Ctrl+C** â€” Exit

Pressing **K** now shows a full in-app reference: main hotkeys, settings hotkeys, and CLI flags with usage examples.

**Keyboard shortcuts (Settings screen â€” `P` key):**
- **â†‘â†“** â€” Navigate providers, analytics row, maintenance row, and profile rows
- **Enter** â€” Edit API key inline, toggle analytics, check/install update, or load a profile
- **Space** â€” Toggle provider enabled/disabled, or toggle analytics
- **T** â€” Test current provider's API key (fires a live ping)
- **U** â€” Check for updates manually from settings
- **Backspace** â€” Delete the selected profile (only on profile rows)
- **Esc** â€” Close settings and return to main TUI

---

### ğŸ“‹ Config Profiles

Profiles let you save and restore different TUI configurations â€” useful if you switch between work/personal setups, different tier preferences, or want to keep separate favorites lists.

**What's stored in a profile:**
- Favorites (starred models)
- Sort column and direction
- Tier filter
- Ping interval
- API keys

**Saving a profile:**
1. Configure the TUI the way you want (favorites, sort, tier, etc.)
2. Press **Shift+S** â€” an inline prompt appears at the bottom
3. Type a name (e.g. `work`, `fast-only`, `presentation`) and press **Enter**
4. The profile is saved and becomes the active profile (shown as a purple badge in the header)

**Switching profiles:**
- **Shift+P** in the main table â€” cycles through saved profiles (or back to raw config)
- **`--profile <name>`** â€” load a specific profile on startup

**Managing profiles:**
- Open Settings (**P** key) â€” scroll down to the **Profiles** section
- **Enter** on a profile row to load it
- **Backspace** on a profile row to delete it

Profiles are stored inside `~/.free-coding-models.json` under the `profiles` key.

---

## ğŸ”§ Development

```bash
git clone https://github.com/vava-nessa/free-coding-models
cd free-coding-models
npm install
npm start -- YOUR_API_KEY
```

### Releasing a new version

1. Make your changes and commit them with a descriptive message
2. Update `CHANGELOG.md` with the new version entry
3. Bump `"version"` in `package.json` (e.g. `0.1.3` â†’ `0.1.4`)
4. Commit with **just the version number** as the message:

```bash
git add .
git commit -m "0.1.4"
git push
```

The GitHub Actions workflow automatically publishes to npm on every push to `main`.

---

## ğŸ“„ License

MIT Â© [vava](https://github.com/vava-nessa)

---

<p align="center">
  <sub>Built with â˜• and ğŸŒ¹ by <a href="https://github.com/vava-nessa">vava</a></sub>
</p>

## ğŸ“¬ Contribute
We welcome contributions! Feel free to open issues, submit pull requests, or get involved in the project.

**Q:** Can I use this with other providers?
**A:** Yes, the tool is designed to be extensible; see the source for examples of customizing endpoints.

**Q:** How accurate are the latency numbers?
**A:** They represent average round-trip times measured during testing; actual performance may vary based on network conditions.

**Q:** Do I need to download models locally for OpenClaw?
**A:** No â€” `free-coding-models` configures OpenClaw to use NVIDIA NIM's remote API, so models run on NVIDIA's infrastructure. No GPU or local setup required.

## ğŸ“§ Support

For questions or issues, open a [GitHub issue](https://github.com/vava-nessa/free-coding-models/issues).

ğŸ’¬ Let's talk about the project on Discord: https://discord.gg/5MbTnDC3Md
