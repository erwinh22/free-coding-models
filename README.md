<p align="center">
  <img src="https://img.shields.io/npm/v/free-coding-models?color=76b900&label=npm&logo=npm" alt="npm version">
  <img src="https://img.shields.io/node/v/free-coding-models?color=76b900&logo=node.js" alt="node version">
  <img src="https://img.shields.io/npm/l/free-coding-models?color=76b900" alt="license">
  <img src="https://img.shields.io/badge/models-44-76b900?logo=nvidia" alt="models count">
</p>

<h1 align="center">âš¡ Free Coding Models</h1>

<p align="center">
  <strong>Find the fastest coding LLM models in seconds</strong><br>
  <sub>Ping free models from multiple providers â€” pick the best one for OpenCode, Cursor, or any AI coding assistant</sub>
</p>

<p align="center">
  <img src="demo.gif" alt="free-coding-models demo" width="100%">
</p>

<p align="center">
  <a href="#-features">Features</a> â€¢
  <a href="#-requirements">Requirements</a> â€¢
  <a href="#-installation">Installation</a> â€¢
  <a href="#-usage">Usage</a> â€¢
  <a href="#-models">Models</a> â€¢
  <a href="#-how-it-works">How it works</a>
</p>

---

## âœ¨ Features

- **ğŸ¯ Coding-focused** â€” Only LLM models optimized for code generation, not chat or vision
- **ğŸš€ Parallel pings** â€” All 44 models tested simultaneously via native `fetch`
- **ğŸ“Š Real-time animation** â€” Watch latency appear live in alternate screen buffer
- **ğŸ† Smart ranking** â€” Top 3 fastest models highlighted with medals ğŸ¥‡ğŸ¥ˆğŸ¥‰
- **â± Continuous monitoring** â€” Pings all models every 10 seconds forever, never stops
- **ğŸ“ˆ Rolling averages** â€” Avg calculated from ALL successful pings since start
- **ğŸ”„ Auto-retry** â€” Timeout models keep getting retried, nothing is ever "given up on"
- **ğŸ® Interactive selection** â€” Navigate with arrow keys directly in the table, press Enter to launch OpenCode
- **ğŸ”Œ Auto-configuration** â€” Detects NVIDIA NIM setup, installs if missing, sets as default model
- **ğŸ¨ Clean output** â€” Zero scrollback pollution, interface stays open until Ctrl+C
- **ğŸ“¶ Status indicators** â€” UP âœ… Â· Timeout â± Â· Down âŒ
- **ğŸ”§ Multi-source support** â€” Extensible architecture via `sources.js` (add new providers easily)

---

## ğŸ“‹ Requirements

Before using `free-coding-models`, make sure you have:

1. **Node.js 18+** â€” Required for native `fetch` API
2. **OpenCode installed** â€” [Install OpenCode](https://github.com/opencode-ai/opencode) (`npm install -g opencode`)
3. **NVIDIA NIM account** â€” Free tier available at [build.nvidia.com](https://build.nvidia.com)
4. **API key** â€” Generate one from Profile â†’ API Keys â†’ Generate API Key

> ğŸ’¡ **Tip:** Without OpenCode installed, you can still use the tool to benchmark models. OpenCode is only needed for the auto-launch feature.

---

## ğŸ“¦ Installation

```bash
# npm (global install â€” recommended)
npm install -g free-coding-models

# pnpm
pnpm add -g free-coding-models

# bun
bun add -g free-coding-models

# Or use directly with npx/pnpx/bunx
npx free-coding-models YOUR_API_KEY
pnpx free-coding-models YOUR_API_KEY
bunx free-coding-models YOUR_API_KEY
```

---

## ğŸš€ Usage

```bash
# Just run it â€” will prompt for API key if not set
free-ai-opencode
```

**How it works:**
1. **Ping phase** â€” All 44 models are pinged in parallel
2. **Continuous monitoring** â€” Models are re-pinged every 10 seconds forever
3. **Real-time updates** â€” Watch "Latest" and "Avg" columns update live
4. **Select anytime** â€” Use â†‘â†“ arrows to navigate, press Enter on a model to launch OpenCode
5. **Smart detection** â€” Automatically detects if NVIDIA NIM is configured in OpenCode:
   - âœ… If configured â†’ Sets model as default and launches OpenCode
   - âš ï¸ If missing â†’ Shows installation instructions and launches OpenCode

Setup wizard:

```
  ğŸ”‘ Setup your NVIDIA API key
  ğŸ“ Get a free key at: https://build.nvidia.com
  ğŸ’¾ Key will be saved to ~/.free-ai-opencode

  Enter your API key: nvapi-xxxx-xxxx

  âœ… API key saved to ~/.free-ai-opencode
```

### Other ways to provide the key

```bash
# Pass directly
free-ai-opencode nvapi-xxxx-your-key-here

# Use environment variable
NVIDIA_API_KEY=nvapi-xxx free-ai-opencode

# Or add to your shell profile
export NVIDIA_API_KEY=nvapi-xxxx-your-key-here
free-ai-opencode
```

### Get your free API key

1. **Create NVIDIA Account** â€” Sign up at [build.nvidia.com](https://build.nvidia.com) with your email
2. **Verify** â€” Confirm email, set privacy options, create NGC account, verify phone
3. **Generate Key** â€” Go to Profile â†’ API Keys â†’ Generate API Key
4. **Name it** â€” e.g., "free-ai-opencode" or "OpenCode-NIM"
5. **Set expiration** â€” Choose "Never" for convenience
6. **Copy securely** â€” Key is shown only once!

> ğŸ’¡ **Free credits** â€” NVIDIA offers free credits for NIM models via their API Catalog for developers.

---

## ğŸ¤– Coding Models

**44 coding models** across 8 tiers, ranked by [Aider Polyglot benchmark](https://aider.chat/docs/leaderboards) (225 coding exercises across C++/Go/Java/JS/Python/Rust). Models without a confirmed Aider score are estimated from model family, size, and published release benchmarks.

| Tier | Score | Count | Models |
|------|-------|-------|--------|
| **S+** | 75%+ | 7 | DeepSeek V3.1/Terminus, DeepSeek V3.2, Kimi K2.5, Devstral 2, Nemotron Ultra 253B, Mistral Large 675B |
| **S**  | 62â€“74% | 7 | Qwen2.5 Coder 32B, GLM 5, Qwen3.5 400B VLM, Qwen3 Coder 480B, Qwen3 80B Thinking, Llama 3.1 405B, MiniMax M2.1 |
| **A+** | 54â€“62% | 6 | Kimi K2 Thinking/Instruct, Qwen3 235B, Llama 3.3 70B, GLM 4.7, Qwen3 80B Instruct |
| **A**  | 44â€“54% | 5 | MiniMax M2, Mistral Medium 3, Magistral Small, Nemotron Nano 30B, R1 Distill 32B |
| **A-** | 36â€“44% | 5 | GPT OSS 120B, Nemotron Super 49B, Llama 4 Scout, R1 Distill 14B, Colosseum 355B |
| **B+** | 25â€“36% | 5 | QwQ 32B, GPT OSS 20B, Stockmark 100B, Seed OSS 36B, Step 3.5 Flash |
| **B**  | 14â€“25% | 5 | Llama 4 Maverick, Mixtral 8x22B, Ministral 14B, Granite 34B Code, R1 Distill 8B |
| **C**  | <14%  | 4 | R1 Distill 7B, Gemma 2 9B, Phi 3.5 Mini, Phi 4 Mini |

### Tier scale

- **S+/S** â€” Frontier coders, top Aider polyglot scores, best for complex refactors
- **A+/A** â€” Excellent alternatives, strong at most coding tasks
- **A-/B+** â€” Solid performers, good for targeted programming tasks
- **B/C** â€” Lightweight or older models, good for code completion on constrained infra

---

## ğŸ”Œ Use with OpenCode

**The easiest way** â€” let `free-ai-opencode` do everything:

1. **Run**: `free-ai-opencode`
2. **Wait** for models to be pinged (green âœ… status)
3. **Navigate** with â†‘â†“ arrows to your preferred model
4. **Press Enter** â€” tool automatically:
   - Detects if NVIDIA NIM is configured in OpenCode
   - Sets your selected model as default in `~/.config/opencode/opencode.json`
   - Launches OpenCode with the model ready to use

That's it! No manual config needed.

### Manual Setup (Optional)

If you prefer to configure OpenCode yourself:

#### Prerequisites

1. **OpenCode installed**: `npm install -g opencode` (or equivalent)
2. **NVIDIA NIM account**: Get a free account at [build.nvidia.com](https://build.nvidia.com)
3. **API key generated**: Go to Profile â†’ API Keys â†’ Generate API Key

#### 1. Find your model

Run `free-ai-opencode` to see which models are available and fast. The "Latest" column shows real-time latency, "Avg" shows rolling average.

#### 2. Configure OpenCode

Create or edit `~/.config/opencode/opencode.json`:

```json
{
  "provider": {
    "nvidia": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "NVIDIA NIM",
      "options": {
        "baseURL": "https://integrate.api.nvidia.com/v1",
        "apiKey": "{env:NVIDIA_API_KEY}"
      }
    }
  },
  "model": "nvidia/deepseek-ai/deepseek-v3.2"
}
```

#### 3. Set environment variable

```bash
export NVIDIA_API_KEY=nvapi-xxxx-your-key-here
# Add to ~/.bashrc or ~/.zshrc for persistence
```

#### 4. Use it

Run `/models` in OpenCode and select **NVIDIA NIM** provider and your chosen model.

> âš ï¸ **Note:** Free models have usage limits based on NVIDIA's tier â€” check [build.nvidia.com](https://build.nvidia.com) for quotas.

### Automatic Installation

The tool includes a **smart fallback mechanism**:

1. **Primary**: Try to launch OpenCode with the selected model
2. **Fallback**: If NVIDIA NIM is not detected in `~/.config/opencode/opencode.json`, the tool:
   - Shows installation instructions in your terminal
   - Creates a `prompt` file in `$HOME/prompt` with the exact configuration to add
   - Launches OpenCode, which will detect and display the prompt automatically

This **"prompt" fallback** ensures that even if NVIDIA NIM isn't pre-configured, OpenCode will guide you through installation with the ready-to-use configuration already prepared.

#### Example prompt file created at `$HOME/prompt`:

```json
Please install NVIDIA NIM provider in OpenCode by adding this to ~/.config/opencode/opencode.json:

{
  "provider": {
    "nvidia": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "NVIDIA NIM",
      "options": {
        "baseURL": "https://integrate.api.nvidia.com/v1",
        "apiKey": "{env:NVIDIA_API_KEY}"
      }
    }
  }
}

Then set env var: export NVIDIA_API_KEY=your_key_here
```

OpenCode will automatically detect this file when launched and guide you through the installation.

---

## âš™ï¸ How it works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Enter alternate screen buffer (like vim/htop/less)      â”‚
â”‚  2. Ping ALL models in parallel                             â”‚
â”‚  3. Display real-time table with Latest/Avg columns         â”‚
â”‚  4. Re-ping ALL models every 10 seconds (forever)          â”‚
â”‚  5. Update rolling averages from ALL successful pings      â”‚
â”‚  6. User can navigate with â†‘â†“ and select with Enter       â”‚
â”‚  7. On Enter: stop monitoring, exit alt screen            â”‚
â”‚  8. Detect NVIDIA NIM config in OpenCode                   â”‚
â”‚  9. If configured: update default model, launch OpenCode   â”‚
â”‚ 10. If missing: show install prompt, launch OpenCode      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Result:** Continuous monitoring interface that stays open until you select a model or press Ctrl+C. Rolling averages give you accurate long-term latency data, and you can launch OpenCode with your chosen model in one keystroke.

---

## ğŸ“‹ API Reference

| Parameter | Description |
|-----------|-------------|
| `NVIDIA_API_KEY` | Environment variable for API key |
| `<api-key>` | First positional argument |

**Configuration:**
- **Ping timeout**: 6 seconds per attempt (models slower than this are unusable)
- **Retry policy**: 2 attempts max per ping cycle (12 seconds total before moving to next model)
- **Ping interval**: 10 seconds between complete re-pings of all models
- **Monitor mode**: Interface stays open forever, press Ctrl+C to exit

---

## ğŸ”§ Development

```bash
git clone https://github.com/vava-nessa/free-ai-opencode
cd free-ai-opencode
npm install
npm start -- YOUR_API_KEY
```

---

## ğŸ“„ License

MIT Â© [vava](https://github.com/vava-nessa)

---

<p align="center">
  <sub>Built with â˜• and ğŸŒ¹ by <a href="https://github.com/vava-nessa">vava</a></sub>
</p>

## ğŸ“¬ Contribute
We welcome contributions! Feel free to open issues, submit pull requests, or get involved in the project.

**Q:** Can I use this with other providers?
**A:** Yes, the tool is designed to be extensible; see the source for examples of customizing endpoints.

**Q:** How accurate are the latency numbers?
**A:** They represent average round-trip times measured during testing; actual performance may vary based on network conditions.

## ğŸ“§ Support
For questions or issues, open a GitHub issue or join our community Discord: https://discord.gg/free-ai-opencode
