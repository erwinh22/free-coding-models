<p align="center">
  <img src="https://img.shields.io/npm/v/nimping?color=76b900&label=npm&logo=npm" alt="npm version">
  <img src="https://img.shields.io/node/v/nimping?color=76b900&logo=node.js" alt="node version">
  <img src="https://img.shields.io/npm/l/nimping?color=76b900" alt="license">
  <img src="https://img.shields.io/badge/models-44-76b900?logo=nvidia" alt="models count">
</p>

<h1 align="center">âš¡ nimping</h1>

<p align="center">
  <strong>Find the fastest NVIDIA NIM coding models in seconds</strong><br>
  <sub>Ping 44 free LLM models optimized for code â€” pick the best one for OpenCode, Cursor, or any AI coding assistant</sub>
</p>

<p align="center">
  <img src="demo.gif" alt="nimping demo" width="300">
</p>

<p align="center">
  <a href="#-features">Features</a> â€¢
  <a href="#-installation">Installation</a> â€¢
  <a href="#-usage">Usage</a> â€¢
  <a href="#-models">Models</a> â€¢
  <a href="#-how-it-works">How it works</a>
</p>

---

## âœ¨ Features

- **ğŸ¯ Coding-focused** â€” Only LLM models optimized for code generation, not chat or vision
- **ğŸš€ Parallel pings** â€” All 44 models tested simultaneously via native `fetch`
- **ğŸ“Š Real-time animation** â€” Watch latency appear live in alternate screen buffer
- **ğŸ† Smart ranking** â€” Top 3 fastest models highlighted with medals ğŸ¥‡ğŸ¥ˆğŸ¥‰
- **â± 4x reliability** â€” Each UP model gets 4 pings for accurate average latency
- **ğŸ¨ Clean output** â€” Zero scrollback pollution, only final table remains
- **ğŸ“¶ Status indicators** â€” UP âœ… Â· Timeout â± Â· Down âŒ

---

## ğŸ“¦ Installation

```bash
# npm (global install â€” recommended)
npm install -g nimping

# pnpm
pnpm add -g nimping

# bun
bun add -g nimping

# Or use directly with npx/pnpx/bunx
npx nimping YOUR_API_KEY
pnpx nimping YOUR_API_KEY
bunx nimping YOUR_API_KEY
```

**Requirements:** Node.js 18+

---

## ğŸš€ Usage

```bash
# Just run it â€” will prompt for API key if not set
nimping
```

Setup wizard:

```
  ğŸ”‘ Setup your NVIDIA API key
  ğŸ“ Get a free key at: https://build.nvidia.com
  ğŸ’¾ Key will be saved to ~/.nimping

  Enter your API key: nvapi-xxxx-xxxx

  âœ… API key saved to ~/.nimping
```

### Other ways to provide the key

```bash
# Pass directly
nimping nvapi-xxxx-your-key-here

# Use environment variable
NVIDIA_API_KEY=nvapi-xxx nimping

# Or add to your shell profile
export NVIDIA_API_KEY=nvapi-xxxx-your-key-here
nimping
```

### Get your free API key

1. **Create NVIDIA Account** â€” Sign up at [build.nvidia.com](https://build.nvidia.com) with your email
2. **Verify** â€” Confirm email, set privacy options, create NGC account, verify phone
3. **Generate Key** â€” Go to Profile â†’ API Keys â†’ Generate API Key
4. **Name it** â€” e.g., "nimping" or "OpenCode-NIM"
5. **Set expiration** â€” Choose "Never" for convenience
6. **Copy securely** â€” Key is shown only once!

> ğŸ’¡ **Free credits** â€” NVIDIA offers free credits for NIM models via their API Catalog for developers.

---

## ğŸ¤– Coding Models

**44 coding models** across 4 tiers, sorted by code generation capability:

| Tier | Count | Models |
|------|-------|--------|
| **S** | 11 | Kimi K2.5, GLM 5, Qwen3 Coder 480B, Qwen3.5 400B VLM, Nemotron Nano 30B, DeepSeek V3.2, Nemotron Ultra 253B, Mistral Large 675B, Qwen3 235B, MiniMax M2.1, Devstral 2 |
| **A** | 13 | GLM 4.7, Kimi K2 Thinking/Instruct, DeepSeek V3.1/Terminus, R1 Distill 14B, QwQ 32B, Qwen3 80B Thinking/Instruct, Qwen2.5 Coder 32B, MiniMax M2, Mistral Medium 3, Magistral Small |
| **B** | 11 | Llama 4 Maverick/Scout, Llama 3.1 405B, Llama 3.3 70B, Nemotron Super 49B, R1 Distill 32B/8B, Colosseum 355B, GPT OSS 120B/20B, Stockmark 100B |
| **C** | 9 | R1 Distill 7B, Seed OSS 36B, Step 3.5 Flash, Mixtral 8x22B, Ministral 14B, Granite 34B Code, Gemma 2 9B, Phi 3.5 Mini, Phi 4 Mini |

### Why these models?

- **S-tier:** Best for coding â€” frontier models with top code generation & reasoning
- **A-tier:** Great alternatives â€” often faster, strong at code tasks
- **B-tier:** Solid coders â€” good for specific programming tasks
- **C-tier:** Lightweight â€” smaller models, edge-friendly for code completion

---

## ğŸ”Œ Use with OpenCode

Want to use NVIDIA NIM models in [OpenCode](https://github.com/opencode-ai/opencode)? Here's how:

### 1. Find your model

Run `nimping` to see which models are available and fast. Pick one that suits you (e.g., `meta/llama-3.1-70b-instruct`, `deepseek-ai/deepseek-v3.2`, `moonshotai/kimi-k2-instruct`).

### 2. Configure OpenCode

Run OpenCode and type `/connect`. Scroll to **"Other"** (custom OpenAI-compatible providers), enter ID `nim`, then paste your NVIDIA API key.

### 3. Edit config

Create or edit `~/.config/opencode/opencode.json`:

```json
{
  "providers": {
    "nim": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "NVIDIA NIM",
      "options": {
        "baseURL": "https://integrate.api.nvidia.com/v1",
        "apiKey": "env:NVIDIA_NIM_API_KEY"
      },
      "models": {
        "kimi": {
          "id": "moonshotai/kimi-k2.5"
        },
        "deepseek": {
          "id": "deepseek-ai/deepseek-v3.2"
        },
        "llama": {
          "id": "meta/llama-3.3-70b-instruct"
        }
      }
    }
  }
}
```

### 4. Set environment variable

```bash
export NVIDIA_NIM_API_KEY=nvapi-xxxx-your-key-here
# Add to ~/.bashrc or ~/.zshrc for persistence
```

### 5. Use it

Run `/models` in OpenCode and select **NVIDIA NIM > kimi** (or your chosen model). Done!

> âš ï¸ **Note:** Free models have usage limits based on NVIDIA's tier â€” check [build.nvidia.com](https://build.nvidia.com) for quotas.

---

## âš™ï¸ How it works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Enter alternate screen buffer (like vim/htop/less)      â”‚
â”‚  2. Ping ALL models in parallel                             â”‚
â”‚  3. Re-ping UP models 3 more times for latency reliability  â”‚
â”‚  4. Exit alternate screen                                   â”‚
â”‚  5. Print final sorted table to stdout (stays in history)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Result:** Clean terminal history with just the final table â€” no animation garbage.

---

## ğŸ“‹ API Reference

| Parameter | Description |
|-----------|-------------|
| `NVIDIA_API_KEY` | Environment variable for API key |
| `<api-key>` | First positional argument |

---

## ğŸ”§ Development

```bash
git clone https://github.com/vava-nessa/nimping
cd nimping
npm install
npm start -- YOUR_API_KEY
```

---

## ğŸ“„ License

MIT Â© [vava](https://github.com/vava-nessa)

---
 
<p align="center">
  <sub>Built with â˜• and ğŸŒ¹ by <a href="https://github.com/vava-nessa">vava</a></sub>
</p>

## ğŸ›  Contributing
We welcome contributions! Please read our [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines on how to propose changes, file issues, or submit pull requests. All contributions are subject to the Code of Conduct.

## ğŸ“š FAQ
**Q:** Do I need a paid NVIDIA account?  
**A:** No, free accounts have free tier credits, but usage is limited. Check [build.nvidia.com](https://build.nvidia.com) for current quotas and limitations.

**Q:** Can I use this with other providers?  
**A:** Yes, the tool is designed to be extensible; see the source for examples of customizing endpoints.

**Q:** How accurate are the latency numbers?  
**A:** They represent average round-trip times measured during testing; actual performance may vary based on network conditions.

## ğŸ“§ Support
For questions or issues, open a GitHub issue or join our community Discord: https://discord.gg/nimping
